{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aElr2Glq-yd"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "Ce notebook ne doit contenir que votre script servant à l'entrainement de votre modèle. Nous devons pouvoir l'exécuter en cliquant sur *Exécution -> Tout exécuter*.\n",
    "\n",
    "Veuillez également ajouter des commentaires dans votre code pour expliquer ce que vous faites. N'hésitez pas à ajouter des blocs de textes (cliquez sur le bouton *+ Texte* en dessous du menu) pour ajouter plus d'explications.\n",
    "\n",
    "Vous devrez déposer sur Moodle une archive au format .zip contenant un dossier avec vos noms.\n",
    "\n",
    "Dans ce dossier, nous devons retrouver les deux notebooks (training et testing) ainsi qu'un nouveau dossier *models* contenant les poids de vos modèles entrainés, et si nécessaire un dossier *datasets* contenant d'autres données utilisée pour effectuer l'apprentissage de vos modèles (données obtenues par récupération sur le web \"web scraping\"  ou bien augmentation de données \"data augmentation\"). Si vous effectuez de l'augmentation de données, fournissez aussi le code pour la réaliser dans le notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Importation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\odanl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding, pipeline, logging\n",
    "import evaluate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#np.set_printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan', precision=3, suppress=False, threshold=1000, formatter=None)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "#logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Augmentation de données par vecteur le plus proche\n",
    "\n",
    "https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model_fr = spacy.load(\"fr_core_news_sm\")\n",
    "model = KeyedVectors.load_word2vec_format(\"frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin\", binary=True, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_vector(word):\n",
    "    try:\n",
    "        similar_words = model.most_similar(word)\n",
    "        return [w[0] for w in similar_words]\n",
    "    except KeyError:\n",
    "        return [word]\n",
    "\n",
    "# Function to augment dataset\n",
    "def augmented_vector_data(dataset):\n",
    "    augmented_sentences = []\n",
    "    for d in dataset:\n",
    "        augmented_sentences.append(d['data'])\n",
    "\n",
    "    for d in dataset:\n",
    "        sentence = d['data']\n",
    "        spacy_analysed = spacy_model_fr(sentence)\n",
    "        augmented_sentence = []\n",
    "\n",
    "        for token in spacy_analysed:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nearest = find_nearest_vector(f\"{token.text}\")\n",
    "                augmented_sentence.append(nearest[0])\n",
    "            else:\n",
    "                augmented_sentence.append(token.text)\n",
    "\n",
    "        augmented_sentences.append(' '.join(augmented_sentence))\n",
    "\n",
    "    ids = list(range(0, len(augmented_sentences)))\n",
    "\n",
    "    # Prepare DataFrame to save\n",
    "    data = {\n",
    "        \"\": ids,\n",
    "        \"data\": augmented_sentences,\n",
    "        \"label\": [d['label'] for d in dataset] * 2,\n",
    "        \"target_name\": [d['target_name'] for d in dataset] * 2\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"augmented_vector_train.csv\", index=False)\n",
    "    print(\"CSV file created: augmented_vector_train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Augmentation de données avec model de language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_mask_data(dataset):\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    lm_unmasker = pipeline('fill-mask', model='allenai/longformer-base-4096', device=device)\n",
    "    augmented_sentences = []\n",
    "    for d in dataset:\n",
    "        augmented_sentences.append(d['data'])\n",
    "\n",
    "    for j,d in enumerate(dataset):\n",
    "        sentence = d['data']\n",
    "        spacy_analysed = spacy_model_fr(sentence)\n",
    "        augmented_sentence = []\n",
    "\n",
    "        for i, token in enumerate(spacy_analysed):\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                temp = ' '.join([t.text for t in spacy_analysed[:i]]) + \" <mask> \" + ' '.join([t.text for t in spacy_analysed[i+1:]])\n",
    "                output = lm_unmasker(temp)\n",
    "                augmented_sentence.append(output[0]['token_str'])\n",
    "            else:\n",
    "                augmented_sentence.append(token.text)\n",
    "\n",
    "\n",
    "        print(f\"{j+1}/{len(dataset)+1}\",' '.join(augmented_sentence))\n",
    "        augmented_sentences.append(' '.join(augmented_sentence))\n",
    "\n",
    "    \n",
    "    ids = list(range(0, len(augmented_sentences)))\n",
    "\n",
    "    # Prepare DataFrame to save\n",
    "    data = {\n",
    "        \"\": ids,\n",
    "        \"data\": augmented_sentences,\n",
    "        \"label\": [d['label'] for d in dataset] * 2,\n",
    "        \"target_name\": [d['target_name'] for d in dataset] * 2\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"augmented_mask_train.csv\", index=False)\n",
    "    print(\"CSV file created: augmented_mask_train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augment_methods = \"vector_data\" #@param [\"basic_data\",\"vector_data\",\"mask_data\"]\n",
    "basic_data_path = 'fake_train.csv'\n",
    "basic_dataset = Dataset.from_pandas(pd.read_csv(basic_data_path))\n",
    "\n",
    "match(data_augment_methods):\n",
    "    case 'basic_data':\n",
    "        dataset = basic_dataset\n",
    "    case 'vector_data':\n",
    "        augmented_file_vector_path = 'augmented_vector_train.csv'\n",
    "        if not os.path.exists(augmented_file_vector_path):\n",
    "            augmented_vector_data(basic_dataset)\n",
    "        dataset = Dataset.from_pandas(pd.read_csv(augmented_file_vector_path))\n",
    "    case 'mask_data': # à éviter prend bcp de temps :sob: (genre +5h avec ma rtx3060ti)\n",
    "        augmented_file_mask_path = 'augmented_mask_train.csv'\n",
    "        if not os.path.exists(augmented_file_mask_path):\n",
    "            augmented_mask_data(basic_dataset)\n",
    "        dataset = Dataset.from_pandas(pd.read_csv(augmented_file_mask_path))\n",
    "\n",
    "ds_train, ds_test = dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transformer model Roberta_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Définition du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\odanl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"News\", 1: \"Fake News\"}\n",
    "label2id = {\"News\": 0, \"Fake News\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"klue/roberta-small\",\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2332/2332 [00:00<00:00, 3001.57 examples/s]\n",
      "Map: 100%|██████████| 584/584 [00:00<00:00, 2969.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['data'], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "tokenized_train = ds_train.map(preprocess_function, batched=True)\n",
    "tokenized_test = ds_test.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Lancement du train pour Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 20%|██        | 146/730 [01:32<03:42,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36216431856155396, 'eval_accuracy': 0.8424657534246576, 'eval_runtime': 34.7971, 'eval_samples_per_second': 16.783, 'eval_steps_per_second': 0.287, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 292/730 [03:16<03:07,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21409215033054352, 'eval_accuracy': 0.9041095890410958, 'eval_runtime': 30.8541, 'eval_samples_per_second': 18.928, 'eval_steps_per_second': 0.324, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 438/730 [05:49<03:44,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25225746631622314, 'eval_accuracy': 0.910958904109589, 'eval_runtime': 56.0042, 'eval_samples_per_second': 10.428, 'eval_steps_per_second': 0.179, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 500/730 [06:39<02:49,  1.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3332, 'learning_rate': 6.301369863013699e-06, 'epoch': 3.42}\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'defi_3_model\\\\checkpoint-500'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# pour save un backup mais bug sur mon pc\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#    save_strategy=\"epoch\",\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#    load_best_model_at_end=True,\u001b[39;00m\n\u001b[0;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     18\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\odanl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\odanl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1929\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\odanl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2300\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2297\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[0;32m   2299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\odanl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2418\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2415\u001b[0m os\u001b[38;5;241m.\u001b[39mrename(staging_output_dir, output_dir)\n\u001b[0;32m   2417\u001b[0m \u001b[38;5;66;03m# Ensure rename completed in cases where os.rename is not atomic\u001b[39;00m\n\u001b[1;32m-> 2418\u001b[0m fd \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mO_RDONLY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2419\u001b[0m os\u001b[38;5;241m.\u001b[39mfsync(fd)\n\u001b[0;32m   2420\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(fd)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'defi_3_model\\\\checkpoint-500'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"defi_3_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# pour save un backup mais bug sur mon pc\n",
    "#    save_strategy=\"epoch\",\n",
    "#    load_best_model_at_end=True,\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"saved_model_rob_small\")\n",
    "tokenizer.save_pretrained(\"saved_model_rob_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1. creation du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(\"fake_train.csv\"))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)  # Use top 10,000 words\n",
    "\n",
    "tokenizer.fit_on_texts(dataset['data'])\n",
    "sequences = tokenizer.texts_to_sequences(dataset['data'])\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "max_length = 1000  # Maximum sequence length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, np.array(dataset['label']), test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\odanl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.5065 - loss: 0.6863 - val_accuracy: 0.6267 - val_loss: 0.6516\n",
      "Epoch 2/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7418 - loss: 0.6087 - val_accuracy: 0.9075 - val_loss: 0.5695\n",
      "Epoch 3/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8045 - loss: 0.5082 - val_accuracy: 0.8151 - val_loss: 0.4592\n",
      "Epoch 4/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8522 - loss: 0.3767 - val_accuracy: 0.8836 - val_loss: 0.3665\n",
      "Epoch 5/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8990 - loss: 0.2861 - val_accuracy: 0.9315 - val_loss: 0.2964\n",
      "Epoch 6/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9450 - loss: 0.1730 - val_accuracy: 0.9418 - val_loss: 0.2325\n",
      "Epoch 7/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9745 - loss: 0.1123 - val_accuracy: 0.9281 - val_loss: 0.1972\n",
      "Epoch 8/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9875 - loss: 0.0654 - val_accuracy: 0.9349 - val_loss: 0.1961\n",
      "Epoch 9/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9885 - loss: 0.0615 - val_accuracy: 0.9110 - val_loss: 0.2431\n",
      "Epoch 10/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9953 - loss: 0.0384 - val_accuracy: 0.9247 - val_loss: 0.2078\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9665 - loss: 0.0968\n",
      "Validation Accuracy: 93.21%\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Dataset.from_pandas(pd.read_csv(\"fake_test.csv\"))\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(test_dataset['data'])\n",
    "tokenizer.fit_on_texts(test_dataset['data'])\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "loss, accuracy = model.evaluate(padded_sequences_test, np.array(test_dataset['label']))\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

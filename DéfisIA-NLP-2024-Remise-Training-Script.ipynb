{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aElr2Glq-yd"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "Ce notebook ne doit contenir que votre script servant à l'entrainement de votre modèle. Nous devons pouvoir l'exécuter en cliquant sur *Exécution -> Tout exécuter*.\n",
    "\n",
    "Veuillez également ajouter des commentaires dans votre code pour expliquer ce que vous faites. N'hésitez pas à ajouter des blocs de textes (cliquez sur le bouton *+ Texte* en dessous du menu) pour ajouter plus d'explications.\n",
    "\n",
    "Vous devrez déposer sur Moodle une archive au format .zip contenant un dossier avec vos noms.\n",
    "\n",
    "Dans ce dossier, nous devons retrouver les deux notebooks (training et testing) ainsi qu'un nouveau dossier *models* contenant les poids de vos modèles entrainés, et si nécessaire un dossier *datasets* contenant d'autres données utilisée pour effectuer l'apprentissage de vos modèles (données obtenues par récupération sur le web \"web scraping\"  ou bien augmentation de données \"data augmentation\"). Si vous effectuez de l'augmentation de données, fournissez aussi le code pour la réaliser dans le notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Importation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\odanl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding, pipeline, logging\n",
    "import evaluate\n",
    "\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "np.set_printoptions(edgeitems=3, infstr='inf', linewidth=150, nanstr='nan', precision=3, suppress=False, threshold=1000, formatter=None)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Augmentation de données par vecteur le plus proche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model_fr = spacy.load(\"fr_core_news_sm\")\n",
    "model = KeyedVectors.load_word2vec_format(\"frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin\", binary=True, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_vector(word):\n",
    "    try:\n",
    "        similar_words = model.most_similar(word)\n",
    "        return [w[0] for w in similar_words]\n",
    "    except KeyError:\n",
    "        return [word]\n",
    "\n",
    "# Function to augment dataset\n",
    "def augmented_vector_data(dataset):\n",
    "    augmented_sentences = []\n",
    "    for d in dataset:\n",
    "        augmented_sentences.append(d['data'])\n",
    "\n",
    "    for d in dataset:\n",
    "        sentence = d['data']\n",
    "        spacy_analysed = spacy_model_fr(sentence)\n",
    "        augmented_sentence = []\n",
    "\n",
    "        for token in spacy_analysed:\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nearest = find_nearest_vector(f\"{token.text}\")\n",
    "                augmented_sentence.append(nearest[0])\n",
    "            else:\n",
    "                augmented_sentence.append(token.text)\n",
    "\n",
    "        augmented_sentences.append(' '.join(augmented_sentence))\n",
    "\n",
    "    ids = list(range(0, len(augmented_sentences)))\n",
    "\n",
    "    # Prepare DataFrame to save\n",
    "    data = {\n",
    "        \"\": ids,\n",
    "        \"data\": augmented_sentences,\n",
    "        \"label\": [d['label'] for d in dataset] * 2,\n",
    "        \"target_name\": [d['target_name'] for d in dataset] * 2\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"augmented_vector_train.csv\", index=False)\n",
    "    print(\"CSV file created: augmented_vector_train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Augmentation de données avec model de language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_mask_data(dataset):\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    lm_unmasker = pipeline('fill-mask', model='allenai/longformer-base-4096', device=device)\n",
    "    augmented_sentences = []\n",
    "    for d in dataset:\n",
    "        augmented_sentences.append(d['data'])\n",
    "\n",
    "    for j,d in enumerate(dataset):\n",
    "        sentence = d['data']\n",
    "        spacy_analysed = spacy_model_fr(sentence)\n",
    "        augmented_sentence = []\n",
    "\n",
    "        for i, token in enumerate(spacy_analysed):\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                temp = ' '.join([t.text for t in spacy_analysed[:i]]) + \" <mask> \" + ' '.join([t.text for t in spacy_analysed[i+1:]])\n",
    "                output = lm_unmasker(temp)\n",
    "                augmented_sentence.append(output[0]['token_str'])\n",
    "            else:\n",
    "                augmented_sentence.append(token.text)\n",
    "\n",
    "\n",
    "        print(f\"{j+1}/{len(dataset)+1}\",' '.join(augmented_sentence))\n",
    "        augmented_sentences.append(' '.join(augmented_sentence))\n",
    "\n",
    "    \n",
    "    ids = list(range(0, len(augmented_sentences)))\n",
    "\n",
    "    # Prepare DataFrame to save\n",
    "    data = {\n",
    "        \"\": ids,\n",
    "        \"data\": augmented_sentences,\n",
    "        \"label\": [d['label'] for d in dataset] * 2,\n",
    "        \"target_name\": [d['target_name'] for d in dataset] * 2\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"augmented_mask_train.csv\", index=False)\n",
    "    print(\"CSV file created: augmented_mask_train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augment_methods = \"vector_data\" #@param [\"basic_data\",\"vector_data\",\"mask_data\"]\n",
    "basic_data_path = 'fake_train.csv'\n",
    "basic_dataset = Dataset.from_pandas(pd.read_csv(basic_data_path))\n",
    "\n",
    "match(data_augment_methods):\n",
    "    case 'basic_data':\n",
    "        dataset = basic_dataset\n",
    "    case 'vector_data':\n",
    "        augmented_file_vector_path = 'augmented_vector_train.csv'\n",
    "        if not os.path.exists(augmented_file_vector_path):\n",
    "            augmented_vector_data(basic_dataset)\n",
    "        dataset = Dataset.from_pandas(pd.read_csv(augmented_file_vector_path))\n",
    "    case 'mask_data': # à éviter prend bcp de temps :sob: (genre +5h avec ma rtx3060ti)\n",
    "        augmented_file_mask_path = 'augmented_mask_train.csv'\n",
    "        if not os.path.exists(augmented_file_mask_path):\n",
    "            augmented_mask_data(basic_dataset)\n",
    "        dataset = Dataset.from_pandas(pd.read_csv(augmented_file_mask_path))\n",
    "\n",
    "ds_train, ds_test = dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transformer model Roberta_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Définition du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"News\", 1: \"Fake News\"}\n",
    "label2id = {\"News\": 0, \"Fake News\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"klue/roberta-small\",\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['data'], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "tokenized_train = ds_train.map(preprocess_function, batched=True)\n",
    "tokenized_test = ds_test.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Lancement du train pour Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"defi_3_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# pour save un backup mais bug sur mon pc\n",
    "#    save_strategy=\"epoch\",\n",
    "#    load_best_model_at_end=True,\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"saved_model_rob_small\")\n",
    "tokenizer.save_pretrained(\"saved_model_rob_small\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
